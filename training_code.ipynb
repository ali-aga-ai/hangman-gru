{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ac001c9",
   "metadata": {},
   "source": [
    "### TRAINING DATA CREATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03868022",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import string\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import multiprocessing\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "MAX_WRONG = 6\n",
    "BATCH_SIZE = 128\n",
    "EMBED_DIM = 32\n",
    "HIDDEN_DIM = 128\n",
    "LR = 1e-3\n",
    "EPOCHS = 2000\n",
    "MAX_WORD_LEN = 30\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ALPHABET = list(string.ascii_uppercase) # this is an issue, no need to convert to uppercase (change later)\n",
    "LETTER_TO_IDX = {c: i for i, c in enumerate(ALPHABET)}\n",
    "IDX_TO_LETTER = {i: c for c, i in LETTER_TO_IDX.items()}\n",
    "PAD_CHAR = \"_\"\n",
    "CHAR_VOCAB = [PAD_CHAR] + ALPHABET\n",
    "CHAR_TO_IDX = {c: i for i, c in enumerate(CHAR_VOCAB)}\n",
    "VOCAB_SIZE = len(CHAR_VOCAB)\n",
    "\n",
    "def load_words(path=\"words.txt\", min_len=2, max_len=MAX_WORD_LEN):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        words = [line.strip().upper() for line in f if line.strip()]\n",
    "        print(f\"Loaded {len(words)} words from {path}\")\n",
    "   \n",
    "    def clean(w): #thora unnecessary\n",
    "        return \"\".join([c for c in w if c.isalpha()])\n",
    "    \n",
    "    words = [clean(w).upper() for w in words if clean(w)] # again upper is wrong (change later)\n",
    "    words = [w for w in words if min_len <= len(w) <= max_len]\n",
    "    words = sorted(list(set(words)))\n",
    "    words = words[:200000] # kernel dying, change from subset to complete (change later)\n",
    "    return words\n",
    "\n",
    "# revealed_letters is a set of letters u know, make a string\n",
    "def mask_word(word, revealed_letters):\n",
    "    return \"\".join([c if c in revealed_letters else PAD_CHAR for c in word])\n",
    "\n",
    "def encode_masked(masked, max_len=MAX_WORD_LEN):\n",
    "    masked = masked[:max_len] \n",
    "    padded = masked + PAD_CHAR * (max_len - len(masked)) # add padding (this may fuck things up cuz model may treat badly(change later))\n",
    "    return [CHAR_TO_IDX.get(ch, 0) for ch in padded]\n",
    "\n",
    "def encode_wrong_vec(wrong_set): # make a vector from the wrongly guessed stuff\n",
    "    vec = np.zeros(26, dtype=np.float32)\n",
    "    for c in wrong_set:\n",
    "        if c in LETTER_TO_IDX:\n",
    "            vec[LETTER_TO_IDX[c]] = 1.0\n",
    "    return vec\n",
    "\n",
    "# like one-hot encoding but with one 26bit int, efficiency me help\n",
    "def set_to_mask(s):\n",
    "    m = 0\n",
    "    for c in s:\n",
    "        m |= 1 << LETTER_TO_IDX[c]\n",
    "    return m\n",
    "\n",
    "# optimsied version w bitmask\n",
    "def words_matching_mask_fast(masked, candidates, wrong_mask):\n",
    "    res = []\n",
    "    L = len(masked)\n",
    "    for w in candidates:\n",
    "        if len(w) != L: \n",
    "            continue\n",
    "        bad = any(masked[i] != PAD_CHAR and masked[i] != w[i] for i in range(L))\n",
    "        if bad:\n",
    "            continue\n",
    "        if any((wrong_mask >> LETTER_TO_IDX[c]) & 1 for c in w):\n",
    "            continue\n",
    "        res.append(w)\n",
    "    return res\n",
    "\n",
    "def process_word_batch(word_batch, words_by_len, samples_per_word, max_wrong):\n",
    "    \"\"\"process a batch of words to reduce overhead\"\"\"\n",
    "    all_letters = set(ALPHABET)\n",
    "    all_states = []\n",
    "    \n",
    "    for w in word_batch:\n",
    "        states = []\n",
    "        L = len(w)\n",
    "        candidates_len = words_by_len[L]\n",
    "        unique_letters = sorted(set(w))\n",
    "        k = len(unique_letters)\n",
    "        \n",
    "        # Local cache for this word \n",
    "        cache_target = {}\n",
    "\n",
    "        for _ in range(samples_per_word):\n",
    "            # how many revealed letters\n",
    "            reveal_count = random.randint(0, max(0, k-1))\n",
    "            # which revealed letters\n",
    "            revealed = set(random.sample(unique_letters, reveal_count)) if reveal_count > 0 else set()\n",
    "            # alphabets not in word\n",
    "            wrong_pool = list(all_letters - set(w))\n",
    "            wrong_count = random.randint(0, max_wrong)\n",
    "            wrong_set = set(random.sample(wrong_pool, min(wrong_count, len(wrong_pool)))) if wrong_count > 0 else set()\n",
    "\n",
    "            masked = mask_word(w, revealed)\n",
    "            wrong_mask = set_to_mask(wrong_set) # get vector / int (int for bitmask)\n",
    "            revealed_mask = set_to_mask(revealed)\n",
    "            key = (masked, wrong_mask, revealed_mask)\n",
    "\n",
    "            if key in cache_target: # if u already have that mask dont reconstruct its target prob (saves time memoisation)\n",
    "                target_dist = cache_target[key]\n",
    "            else:\n",
    "                candidates = words_matching_mask_fast(masked, candidates_len, wrong_mask) \n",
    "                if not candidates:  # if no words match the masked pattern, fallback to all words of same length that don't use wrong letters\n",
    "                    candidates = [x for x in words_by_len[L] if all(((wrong_mask >> LETTER_TO_IDX[c]) & 1) == 0 for c in x)]\n",
    "\n",
    "                excluded_mask = wrong_mask | revealed_mask\n",
    "                target_dist = np.zeros(26, dtype=np.float32)\n",
    "                for cw in candidates:\n",
    "                    for c in set(cw):\n",
    "                        if ((excluded_mask >> LETTER_TO_IDX[c]) & 1) == 0:\n",
    "                            target_dist[LETTER_TO_IDX[c]] += 1\n",
    "                s = target_dist.sum()\n",
    "                if s > 0:\n",
    "                    target_dist /= s\n",
    "                else:\n",
    "                    rem = [i for i in range(26) if ((excluded_mask >> i) & 1) == 0]\n",
    "                    if rem:\n",
    "                        for i in rem:\n",
    "                            target_dist[i] = 1.0 / len(rem)\n",
    "                cache_target[key] = target_dist  # memoise\n",
    "\n",
    "            states.append({\n",
    "                \"masked\": masked,\n",
    "                \"wrong_set\": wrong_set,\n",
    "                \"target\": target_dist.astype(np.float32),\n",
    "                \"word\": w\n",
    "            })\n",
    "        \n",
    "        all_states.extend(states)\n",
    "    \n",
    "    return all_states\n",
    "\n",
    "def generate_training_states(words, max_wrong=MAX_WRONG, samples_per_word=40, n_jobs=-1):\n",
    "    \n",
    "    print(f\"generating training states with {samples_per_word} samples per word...\") # 40 samples thora overkill but u want a lot of data. \n",
    "    \n",
    "    # build words_by_len dictionary, useful when creating masked states ke targets\n",
    "    words_by_len = defaultdict(list)\n",
    "    for w in words:\n",
    "        words_by_len[len(w)].append(w)\n",
    "    \n",
    "    \n",
    "    if n_jobs == -1:\n",
    "        n_jobs = multiprocessing.cpu_count() # use all pcpu cores. \n",
    "    \n",
    "    # create batches to reduce overhead\n",
    "    batch_size = max(1, len(words) // (n_jobs * 4))  # 4 batches per core for load balancing\n",
    "    word_batches = [words[i:i + batch_size] for i in range(0, len(words), batch_size)]\n",
    "    \n",
    "    print(f\"Using {n_jobs} CPU cores with {len(word_batches)} batches...\")\n",
    "    \n",
    "    # parallel process, n_jobs says kitne cpu use, loky is just used cuz stable or sumn (change this later), tqdm should ideally give a progress bar cuz cant add random print stmts with parallel processing.  \n",
    "    results = Parallel(n_jobs=n_jobs, backend='loky', verbose=0)(\n",
    "        delayed(process_word_batch)(batch, words_by_len, samples_per_word, max_wrong) \n",
    "        for batch in tqdm(word_batches, desc=\"Processing batches\")\n",
    "    )\n",
    "    \n",
    "    # parallel will give u a list of results per batch so return list of states\n",
    "    all_states = []\n",
    "    for states in results:\n",
    "        all_states.extend(states)\n",
    "    \n",
    "    print(f\"Generated {len(all_states)} total training states\")\n",
    "    return all_states\n",
    "\n",
    "def main():\n",
    "    words = load_words(\"words.txt\", min_len=2, max_len=MAX_WORD_LEN)\n",
    "    print(\"words length\", len(words))\n",
    "    \n",
    "    samples_per_word = 40 # change this agar u dont have time (change later)\n",
    "    random.shuffle(words) \n",
    "    split = int(0.8 * len(words))\n",
    "    train_words = words[:split]\n",
    "    test_words = words[split:]\n",
    "    \n",
    "    print(f\"Train words: {len(train_words)}, Test words: {len(test_words)}\")\n",
    "    \n",
    "    \n",
    "    start = time.time()\n",
    "    train_states = generate_training_states(\n",
    "        words, \n",
    "        max_wrong=MAX_WRONG, \n",
    "        samples_per_word=samples_per_word,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    end = time.time()\n",
    "    \n",
    "    print(f\"Time to generate training states: {end - start:.2f} seconds\")\n",
    "    \n",
    "    # Save states\n",
    "    states_file = \"states_all.pkl\"\n",
    "    with open(states_file, \"wb\") as f:\n",
    "        pickle.dump(train_states, f)\n",
    "        print(f\"Saved {len(train_states)} training states to {states_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e207768",
   "metadata": {},
   "source": [
    "### TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e09fb4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "import pickle, torch, os, random, time\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "MAX_WORD_LEN = 30\n",
    "BATCH_SIZE = 2048  # make this larger so that faster \n",
    "EPOCHS = 300 # if no plateau increase (change later)\n",
    "LR = 1e-4 \n",
    "MAX_WRONG = 6 # reduce this to constrain model to learn quicker (idk if this will work (change later))\n",
    "\n",
    "import string\n",
    "ALPHABET = list(string.ascii_uppercase)\n",
    "LETTER_TO_IDX = {c: i for i, c in enumerate(ALPHABET)}\n",
    "IDX_TO_LETTER = {i: c for c, i in LETTER_TO_IDX.items()}\n",
    "PAD_CHAR = \"_\"\n",
    "CHAR_VOCAB = [PAD_CHAR] + ALPHABET\n",
    "CHAR_TO_IDX = {c:i for i,c in enumerate(CHAR_VOCAB)}\n",
    "VOCAB_SIZE = len(CHAR_VOCAB)\n",
    "\n",
    "def encode_masked(masked, max_len=MAX_WORD_LEN):\n",
    "    padded = masked[:max_len] + PAD_CHAR*(max_len-len(masked))\n",
    "    return [CHAR_TO_IDX.get(ch,0) for ch in padded]\n",
    "\n",
    "def encode_wrong_vec(wrong_set):\n",
    "    import numpy as np\n",
    "    vec = np.zeros(26, dtype=np.float32)\n",
    "    for c in wrong_set:\n",
    "        if c in LETTER_TO_IDX:\n",
    "            vec[LETTER_TO_IDX[c]] = 1.0\n",
    "    return vec\n",
    "\n",
    "class HangmanDatasetPreEncoded(Dataset):\n",
    "    def __init__(self, encoded_states):\n",
    "        self.encoded_states = encoded_states\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encoded_states)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        s = self.encoded_states[idx]\n",
    "        return s['X_mask'], s['X_wrong'], s['y']\n",
    "\n",
    "# Main model.\n",
    "class HangmanModel(nn.Module):\n",
    "    def __init__(self, char_vocab_size=VOCAB_SIZE, emb_dim=32, hidden_dim=128, out_dim=26):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(char_vocab_size, emb_dim, padding_idx=0)\n",
    "        self.gru = nn.GRU(emb_dim, hidden_dim, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_dim + out_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, out_dim)\n",
    "    \n",
    "    def forward(self, x_mask, x_wrong):\n",
    "        emb = self.emb(x_mask)\n",
    "        _, h = self.gru(emb)\n",
    "        h = h.squeeze(0)\n",
    "        concat = torch.cat([h, x_wrong], dim=1)\n",
    "        x = F.relu(self.fc1(concat))\n",
    "        logits = self.fc2(x)\n",
    "        return F.log_softmax(logits, dim=1)\n",
    "\n",
    "def soft_target_loss(log_probs, target_probs):\n",
    "    '''the commented out lines are for word-weighted loss, otherwise CE loss'''\n",
    "    # target_classes = target_probs.argmax(dim=1)\n",
    "    # weights = class_weights[target_classes]\n",
    "    # return -(target_probs * log_probs).sum(dim=1).mul(weights).mean()\n",
    "    return -(target_probs * log_probs).sum(dim=1).mean()\n",
    "\n",
    "# Use the masked dataset\n",
    "print(\"Loading states...\") \n",
    "with open(\"states_all.pkl\",\"rb\") as f:\n",
    "    train_states = pickle.load(f)\n",
    "random.shuffle(train_states)\n",
    "\n",
    "# split into train/val\n",
    "split_idx = int(0.9 * len(train_states))\n",
    "train_data = train_states[:split_idx]\n",
    "val_data = train_states[split_idx:]\n",
    "\n",
    "# pre encoding will prevent recomputation in the train run, a one-time investment. Unable to save cuz pkl taking too long. maybe try saving np (change later)\n",
    "print(f\"pre-encoding {len(train_data)} train + {len(val_data)} val states...\")\n",
    "train_encoded = []\n",
    "for i, s in enumerate(train_data):\n",
    "    if i % 100000 == 0:\n",
    "        print(f\"  Encoded train {i}/{len(train_data)}\")\n",
    "    train_encoded.append({\n",
    "        'X_mask': torch.tensor(encode_masked(s[\"masked\"], MAX_WORD_LEN), dtype=torch.long),\n",
    "        'X_wrong': torch.tensor(encode_wrong_vec(s[\"wrong_set\"]), dtype=torch.float32),\n",
    "        'y': torch.tensor(s[\"target\"], dtype=torch.float32)\n",
    "    })\n",
    "\n",
    "val_encoded = []\n",
    "for i, s in enumerate(val_data):\n",
    "    val_encoded.append({\n",
    "        'X_mask': torch.tensor(encode_masked(s[\"masked\"], MAX_WORD_LEN), dtype=torch.long),\n",
    "        'X_wrong': torch.tensor(encode_wrong_vec(s[\"wrong_set\"]), dtype=torch.float32),\n",
    "        'y': torch.tensor(s[\"target\"], dtype=torch.float32)\n",
    "    })\n",
    "\n",
    "#save encoded\n",
    "# with open(\"train_encoded.pkl\",\"wb\") as f:\n",
    "#     pickle.dump(train_encoded,f)\n",
    "# with open(\"val_encoded.pkl\",\"wb\") as f:\n",
    "#     pickle.dump(val_encoded,f)\n",
    "    \n",
    "print(\"Loading pre-encoded states from file...\")\n",
    "train_dataset = HangmanDatasetPreEncoded(train_encoded)\n",
    "val_dataset = HangmanDatasetPreEncoded(val_encoded)\n",
    "\n",
    "# Data Loader apparently does things quicker (shuffle etc)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=8,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=2\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=8,\n",
    "    pin_memory=True\n",
    ")\n",
    "print(f\"Dataset ready: {len(train_data)} train, {len(val_data)} val, batch_size={BATCH_SIZE}\")\n",
    "\n",
    "model = HangmanModel().to(DEVICE)\n",
    "model = nn.DataParallel(model, device_ids=[0,1,2]) # wull help running parallel\n",
    "\n",
    "# apparently ADAMW works better than ADAM, test ADAM if time (change later)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01)\n",
    "\n",
    "# Load checkpoint BEFORE compiling CHANGE FILE LOCATION CUREENTLY WRONG TO TRAIN FROM SCRATCH (change later)\n",
    "if os.path.exists(\"hangman_model3150.pth\"):\n",
    "    checkpoint = torch.load(\"hangman_model350.pth\", map_location=DEVICE)\n",
    "    \n",
    "    state_dict = checkpoint[\"model_state\"]\n",
    "    new_state_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        new_key = k.replace(\"_orig_mod.\", \"\")\n",
    "        new_state_dict[new_key] = v\n",
    "    \n",
    "    model.load_state_dict(new_state_dict)\n",
    "    start_epoch = checkpoint.get(\"epoch\", 550)+1\n",
    "    print(f\"Resuming from epoch {start_epoch}\")\n",
    "else:\n",
    "    start_epoch = 1\n",
    "\n",
    "# sleep for 10 seconds kernel crash maybe\n",
    "time.sleep(30)\n",
    "print(\"Starting compile...\") # compile improves model executiuon time, commented out cuz kernel crash for complete dataset\n",
    "# Compile model for speed AFTER loading checkpoint (PyTorch 2.0+)\n",
    "# try:\n",
    "#     model = torch.compile(model, mode='max-autotune')\n",
    "#     print(\"Model compiled with torch.compile\")\n",
    "# except:\n",
    "#     print(\"torch.compile not available, skipping\")\n",
    "\n",
    "time.sleep(20)\n",
    "print(\"starting scaler...\"\n",
    "      )\n",
    "scaler = GradScaler() # syntax issue (scaler reduces fp precision without costing much accuracy)\n",
    "\n",
    "print(\"Starting scheduler\") # adjust the LR accordingly to emperically proven results. HEre some COSINE Curve, can try other methods (CHANGE LATER), the args for this are based on how often u wanna restart the cosine curve of LEARNING\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer, T_0=50, T_mult=2, eta_min=1e-6\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(start_epoch, EPOCHS+1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    epoch_start = time.time()\n",
    "    data_time = 0\n",
    "    train_time = 0\n",
    "    all_logits = []\n",
    "    all_targets = []\n",
    "    \n",
    "    batch_start = time.time()\n",
    "\n",
    "    for batch_idx, (X_mask, X_wrong, y) in enumerate(train_loader):\n",
    "        data_time += time.time() - batch_start\n",
    "        train_start = time.time()\n",
    "        \n",
    "        # the non blocking will ensure that data loasing doesnt overlap with GPU computaiton\n",
    "        X_mask = X_mask.to(DEVICE, non_blocking=True)\n",
    "        X_wrong = X_wrong.to(DEVICE, non_blocking=True)\n",
    "        y = y.to(DEVICE, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with autocast(): # this line is for some scaler shit\n",
    "            logp = model(X_mask, X_wrong)\n",
    "            loss = soft_target_loss(logp, y)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        \n",
    "        # grad clip for explding and vanish grads\n",
    "        total_grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        total_loss += loss.item() * X_mask.size(0)\n",
    "        \n",
    "        if batch_idx == 0:\n",
    "            all_logits.append(logp.detach().cpu())\n",
    "            all_targets.append(y.detach().cpu())\n",
    "        \n",
    "        train_time += time.time() - train_start\n",
    "        batch_start = time.time()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataset)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_per_class = torch.zeros(26)\n",
    "    total_per_class = torch.zeros(26)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_mask, X_wrong, y in val_loader:\n",
    "            X_mask = X_mask.to(DEVICE, non_blocking=True)\n",
    "            X_wrong = X_wrong.to(DEVICE, non_blocking=True)\n",
    "            y = y.to(DEVICE, non_blocking=True)\n",
    "            \n",
    "            with autocast():\n",
    "                logp = model(X_mask, X_wrong)\n",
    "                loss = soft_target_loss(logp, y)\n",
    "            \n",
    "            val_loss += loss.item() * X_mask.size(0)\n",
    "            \n",
    "            # Per-class accuracy (note that this accuracy term may not be a good repr )\n",
    "            preds = logp.argmax(dim=1).cpu()\n",
    "            targets = y.argmax(dim=1).cpu()\n",
    "            for cls in range(26):\n",
    "                mask = (targets == cls)\n",
    "                total_per_class[cls] += mask.sum().item()\n",
    "                correct_per_class[cls] += ((preds == targets) & mask).sum().item()\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_dataset)\n",
    "    scheduler.step()\n",
    "    epoch_time = time.time() - epoch_start\n",
    "    \n",
    "    #log\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"\\nEpoch {epoch}/{EPOCHS}\")\n",
    "    print(f\"  Train Loss: {avg_train_loss:.6f}  Val Loss: {avg_val_loss:.6f}  LR: {current_lr:.6f}\")\n",
    "    print(f\"  Total Grad Norm: {total_grad_norm:.4f}\")\n",
    "    \n",
    "    #track gradient to change LR scheduler etc\n",
    "    if epoch % 10 == 0:\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                grad_norm = param.grad.norm().item()\n",
    "                print(f\"    {name}: {grad_norm:.4f}\")\n",
    "    \n",
    "    # Logit histogram\n",
    "    if len(all_logits) > 0:\n",
    "        logits_sample = torch.cat(all_logits, dim=0)\n",
    "        print(f\"stats: min={logits_sample.min():.3f}, max={logits_sample.max():.3f}, mean={logits_sample.mean():.3f}\")\n",
    "    \n",
    "    # Per-class accuracy (top 5 and bottom 5)\n",
    "    per_class_acc = correct_per_class / (total_per_class + 1e-8)\n",
    "    sorted_idx = per_class_acc.argsort(descending=True)\n",
    "    print(f\"  Top 5 classes: \", end=\"\")\n",
    "    for idx in sorted_idx[:5]:\n",
    "        if total_per_class[idx] > 0:\n",
    "            print(f\"{ALPHABET[idx]}:{per_class_acc[idx]:.3f} \", end=\"\")\n",
    "    print(f\"\\n  Bottom 5 classes: \", end=\"\")\n",
    "    for idx in sorted_idx[-5:]:\n",
    "        if total_per_class[idx] > 0:\n",
    "            print(f\"{ALPHABET[idx]}:{per_class_acc[idx]:.3f} \", end=\"\")\n",
    "    print()\n",
    "\n",
    "\n",
    "    #save checkpoint every 50 epochs\n",
    "    if epoch % 50 == 0:\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"optimizer_state\": optimizer.state_dict(),\n",
    "            \"scheduler_state\": scheduler.state_dict(),\n",
    "            \"CHAR_VOCAB\": CHAR_VOCAB,\n",
    "            \"LETTER_TO_IDX\": LETTER_TO_IDX\n",
    "        }, f\"hangman_model{epoch}.pth\")\n",
    "        print(f\"  Checkpoint saved: hangman_model{epoch}.pth\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b067ae30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce622799",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028291fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
